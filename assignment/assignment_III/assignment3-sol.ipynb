{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning II 2021-2022 - UMONS \n",
    "# Assignment III\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression data simulation \n",
    "np.random.seed(42)\n",
    "\n",
    "N = 200\n",
    "d = 2\n",
    "\n",
    "s = np.random.uniform(-2,2, (2, 2))\n",
    "s.shape\n",
    "cov = np.dot(s, s.T)\n",
    "mean = (1, 2)\n",
    "X = np.random.multivariate_normal(mean, cov, N)\n",
    "eps = np.random.normal(0, 1, N).reshape(-1, 1)\n",
    "\n",
    "coef = np.random.uniform(-2, 2, (2, 1))\n",
    "y = np.dot(X, coef) + eps\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the least squres solution\n",
    "# w_lin = (X^T X)^-1 X^T y\n",
    "w_lin = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(w_lin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = np.array([-20, -40]).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement gradient descent for linear regression\n",
    "eta = 0.01  # learning rate\n",
    "n_iterations = 100 #1000\n",
    "\n",
    "list_w_gd = []\n",
    "w = w_0  \n",
    "list_w_gd.append(w)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/N * X.T.dot(X.dot(w) - y)\n",
    "    w = w - eta * gradients\n",
    "    list_w_gd.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_w_gd[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement stochastic gradient descentt\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 10\n",
    "eta = 0.01\n",
    "\n",
    "list_w_sgd = []\n",
    "w = w_0  \n",
    "list_w_sgd.append(w)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(N):\n",
    "        random_index = np.random.randint(N)\n",
    "        xi = X[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - eta * gradients\n",
    "        list_w_sgd.append(w)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mini-batch stochastic gradient descentt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 50\n",
    "minibatch_size = 5\n",
    "eta = 0.01\n",
    "\n",
    "list_w_mgd = []\n",
    "w = w_0  \n",
    "list_w_mgd.append(w)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, N, minibatch_size):\n",
    "        xi = X_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(w) - yi)\n",
    "        w = w - eta * gradients\n",
    "        list_w_mgd.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_w_gd = np.array(list_w_gd)\n",
    "list_w_sgd = np.array(list_w_sgd)\n",
    "list_w_mgd = np.array(list_w_mgd)\n",
    "\n",
    "plt.plot(list_w_gd[:, 0], list_w_gd[:, 1], \"b-o\", linewidth=3, label=\"GD\")\n",
    "plt.plot(list_w_sgd[:, 0], list_w_sgd[:, 1], \"r-s\", linewidth=1, label=\"SGD\")\n",
    "plt.plot(list_w_mgd[:, 0], list_w_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.xlabel(r\"$w_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$w_1$   \", fontsize=20, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{D} = \\{(\\mathbf{x}_n, y_n)\\}_{n=1}^N$ where $\\mathbf{x}_n \\in \\mathbb{R}^d$ and $y_n \\in \\{C_1, C_2, \\dots, C_K\\}$. We want to train a $K$-class classifier using linear softmax regression, a generalization of binary logistic regression for $K > 2$. In this model, a hypothesis is given by\n",
    "\n",
    "$$ h_k(\\mathbf{x}_n) = P(Y = C_k | X = \\mathbf{x}_n) = \\text{softmax}(s_k),$$\n",
    "where \n",
    "$$\\text{softmax}(s_k) =  \\frac{ e^{s_k} } {\\sum_{l = 1}^K e^{s_l}},$$ \n",
    "$$ s_k = \\mathbf{w}^T_k \\mathbf{x}_n,$$\n",
    "$\\mathbf{w}_k \\in \\mathbb{R}^d $ and $k = 1, 2, \\dots, K$.\n",
    "\n",
    "The denominator $\\sum_{l = 1}^K e^{s_l}$ is used to normalize all the values into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $y_{nk}$ be the one-hot representation of $y_n$, i.e. $y_{nk} = 1$ if $y_n = C_k$ and 0 otherwise. The in-sample cross-entropy error is given by\n",
    "\n",
    "$$E_{\\text{ini}}(\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K) = - \\dfrac{1}{N}\\sum_{n=1}^{N}\\sum_{k=1}^{K}{y_{nk}\\log\\left(h_k(\\mathbf{x}_n)\\right)},$$\n",
    "\n",
    "where\n",
    "$$\n",
    "h_k(\\mathbf{x}_n) = \\frac{ e^{\\mathbf{w}^T_k \\mathbf{x}_n} } {\\sum_{l = 1}^K e^{\\mathbf{w}^T_l \\mathbf{x}_n}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "We ask to you show that \n",
    "$$ \\frac{\\partial E_{\\text{in}}(\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K) }{\\partial \\mathbf{w}_j} = \\frac1N \\sum_{n=1}^{N}  ( h_{j}(\\mathbf{x}_n) - y_{nj} ) \\mathbf{x}_n, \\quad j = 1, 2, \\dots, K. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: when computing $ \\frac{\\partial h_k(\\mathbf{x}_n)  }{\\partial \\mathbf{w}_j } $, consider the two cases where $j = k$ and $j \\neq k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial h_k(\\mathbf{x}_n)  }{\\partial \\mathbf{w}_j } = h_k(\\mathbf{x}_n) (\\delta_{jk} - h_j(\\mathbf{x}_n)) \\mathbf{x}_n$ where $\\delta_{jk} = 1$ if $j = k$ and 0, otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{W} \\in \\mathbb{R}^{d \\times K}$ be a matrix where the vector $\\mathbf{w}_1, \\dots, \\mathbf{w}_K $ are stacked by column, $\\mathbf{Y} \\in \\{0, 1\\}^{N \\times K}$ where $[\\mathbf{Y}]_{nk} = y_{nk}$, and $\\mathbf{H} \\in [0, 1]^{N \\times K}$ where $[\\mathbf{H}]_{nk} = h_k(\\mathbf{x}_n)$. Then, we can write \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial E_{\\text{in}}(\\mathbf{W}) }{\\partial \\mathbf{W}} =  \\frac{X^{T} (H - Y)}{N}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical issues with the softmax function\n",
    "Naively applying the softmax function can lead to numerical stability issues. Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    return np.exp(s) / np.sum(np.exp(s))\n",
    "\n",
    "s = np.array([1000.0, 2000.0, 3000.0])\n",
    "print(s)\n",
    "print(softmax(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, both the numerator and denominator becomes inifity, and numpy returns nan for the division. There is a nice approach to compute the exact same function with a simple trick:\n",
    "\n",
    "$$\\frac{e^{s_k}}{\\sum_{l=1}^K e^{s_l}} = \\frac{e^{s_k}e^C}{\\sum_{l=1}^K e^{s_l} e^C} = \\frac{e^{s_k + C}}{\\sum_{l=1}^K e^{s_l + C}}$$\n",
    "for some fixed $C \\in \\mathbb{R}$. We can use $C = - max(s_1, s_2, \\dots, s_K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function stable_softmax using the previous trick\n",
    "def stable_softmax(s):\n",
    "    return np.exp(s - max(s)) / np.sum(np.exp(s - max(s)))\n",
    "\n",
    "print(s)\n",
    "print(stable_softmax(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "K = 3\n",
    "d = 2\n",
    "N = 500\n",
    "X, y = make_classification(n_samples=N, n_classes=K, n_features=d, n_informative = d, n_redundant = 0, n_clusters_per_class=1, class_sep = 1, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(X[: , 0], X[: , 1], hue = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "Y = lb.fit_transform(y.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matrix H defined previoussly using the stable_softmax function.\n",
    "# Hint: you can use apply_along_axis from numpy.\n",
    "W = np.random.randn(d, K) \n",
    "H = np.apply_along_axis(stable_softmax, 1, np.dot(X, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute E_in as a function of Y and H\n",
    "E_in = - np.mean(np.mean(Y * np.log(H), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent \n",
    "\n",
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000 #1000\n",
    "\n",
    "list_W_gd, loss = [], []\n",
    "W = np.random.randn(d, K)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    H = np.apply_along_axis(stable_softmax, 1, np.dot(X, W))\n",
    "    gradients = 1/N * X.T.dot(H - Y)\n",
    "    W = W - eta * gradients\n",
    "    loss.append(- np.mean(np.mean(Y * np.log(H), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot E_in as a function of the iteration\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the decision boundaries on the same figure.\n",
    "W = W.T\n",
    "\n",
    "colormap = np.array(['r', 'g', 'b'])\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c=colormap[y])\n",
    "xs1 = np.array([-5.0, 3.0])\n",
    "ys1 = (- (W[0, 0] - W[1, 0]) * xs1) / (W[0, 1] - W[1, 1])\n",
    "\n",
    "xs2 = np.array([-5.0, 3.0])\n",
    "ys2 = (- (W[0, 0] - W[2, 0]) * xs2) / (W[0, 1] - W[2, 1])\n",
    "\n",
    "xs3 = np.array([-0.5, 0.5])\n",
    "ys3 = (- (W[1, 0] - W[2, 0]) * xs3) / (W[1, 1] - W[2, 1])\n",
    "\n",
    "plt.plot(xs1, ys1, c='black')\n",
    "plt.plot(xs2, ys2, c='black')\n",
    "plt.plot(xs3, ys3, c='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
