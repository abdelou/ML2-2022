{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning II 2021-2022 - UMONS \n",
        "# Assignment IV"
      ],
      "metadata": {
        "id": "hKI8Wr5VKQXk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DJHt4zNKMiW"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will get more detailed knowledge on neural networks. You will first implement a neural netowrk without using the PyTorch NN module. Then, you will compare it with the NN module implementation.\n",
        "\n"
      ],
      "metadata": {
        "id": "Df1eBkRTPQQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layer"
      ],
      "metadata": {
        "id": "umh9OoCOP32_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only basic tensor operations\n",
        "**Here you may only use basic tensor operations from the `torch` module: no `torch.nn`, `torch.nn.F` or others.**"
      ],
      "metadata": {
        "id": "w0FZ6BNOXEd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(object):\n",
        "    \"\"\"\n",
        "    Fully connected layer.\n",
        "    \n",
        "    Args:\n",
        "        in_features: number of input features\n",
        "        out_features: number of output features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #      TODO: Define placeholder tensors for layer weight and bias.     #\n",
        "        #       The placeholder tensors should have the correct dimension      #\n",
        "        #        according to the in_features and out_features variables.      #\n",
        "        #                    Note: no for loops are needed!                    #\n",
        "        ########################################################################\n",
        "\n",
        "        self.weight = \n",
        "        self.bias = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_params()\n",
        "\n",
        "        # Define a cache varible to save computation, because some of the\n",
        "        # forward pass values would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "        # Define variables to store the gradients of the weight and bias\n",
        "        # calculated during the backward pass\n",
        "        self.weight_grad = None\n",
        "        self.bias_grad = None\n",
        "\n",
        "    def init_params(self, std=1.):\n",
        "        \"\"\"\n",
        "        Initialize layer parameters. Sample weight from Gaussian distribution\n",
        "        and bias uniform distribution.\n",
        "        \n",
        "        Args:\n",
        "            std: Standard deviation of Gaussian distribution (default: 1.0)\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight = std*torch.randn_like(self.weight)\n",
        "        self.bias = torch.rand_like(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of linear layer: multiply input tensor by weights and add\n",
        "        bias. Store input tensor as cache variable.\n",
        "        \n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #   TODO: Implement this function  and Store input as cache variable   #\n",
        "        ########################################################################\n",
        "\n",
        "        y = \n",
        "        self.cache = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of linear layer: calculate gradients of loss with respect\n",
        "        to weight and bias and return downstream gradient dx.\n",
        "        \n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        x = \n",
        "        dx = \n",
        "        self.weight_grad = \n",
        "        self.bias_grad = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "3H-itMOiPPde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define layer dimensions\n",
        "n_samples, in_features, out_features = 2, 3, 4\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "# Define upstream gradient dL/dy as randn\n",
        "dy = torch.randn((n_samples, out_features))\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Create a layer from the Linear object class above           #\n",
        "#                 and do a forward and a backward pass                 #\n",
        "########################################################################\n",
        "\n",
        "layer = \n",
        "# Forward pass\n",
        "y = \n",
        "# Backward pass\n",
        "dx = \n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# What will be the shape of output tensor y?\n",
        "print('Shape of ouput tensor y:', y.shape)\n",
        "\n",
        "# What will be the shape of gradient of x w.r.t. y?\n",
        "print('Shape of gradient x is:', dx.shape)"
      ],
      "metadata": {
        "id": "JemB6e56Q0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PyTorch NN module\n",
        "\n",
        "Now, you can compare your without PyTorch implementation with PyTorch implementation."
      ],
      "metadata": {
        "id": "6tjkhrdWTBjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the input tensor so we can use x for all other layers\n",
        "# without overwriting its gradients\n",
        "x_lin = x.clone()\n",
        "# Enable requires_grad for x_lin\n",
        "x_lin.requires_grad = True\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Create a layer from the Linear object class above           #\n",
        "#                 and do a forward and a backward pass                 #\n",
        "########################################################################\n",
        "\n",
        "# Create Linear layer from torch.nn module\n",
        "torch_layer = \n",
        "\n",
        "# Load the parameters from our layer into the Pytorch layer\n",
        "torch_layer.weight =  # transpose weight by .t()\n",
        "torch_layer.bias = \n",
        "\n",
        "# Perform forward pass\n",
        "torch_y = \n",
        "\n",
        "# Perform bacward pass\n",
        "\n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "\n",
        "\n",
        "# What will be the shape of output tensor torch_y?\n",
        "print('Shape of ouput tensor torch_y:', torch_y.shape)\n",
        "\n",
        "# What will be the shape of gradient of x w.r.t. y?\n",
        "print('Shape of gradient x is:', dx.shape)"
      ],
      "metadata": {
        "id": "xy3_ORcETDLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare gradients of x, weight, bias w.r.t. y using torch.allclose\n",
        "dx_same = torch.allclose(dx, x_lin.grad)\n",
        "print('dx identical: ', dx_same)\n",
        "dw_same = torch.allclose(layer.weight_grad, torch_layer.weight.grad.T)\n",
        "print('dw identical: ', dw_same)\n",
        "db_same = torch.allclose(layer.bias_grad, torch_layer.bias.grad)\n",
        "print('db identical: ', db_same)"
      ],
      "metadata": {
        "id": "d2qbL4WpUaXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear activation functions\n",
        "\n",
        "Implement ReLU and Sigmoid functions. The Functions are defined as follows:\n",
        "\n",
        "$$\\text{ReLU}(x)=\\max(0,x)\\\\\n",
        "\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$"
      ],
      "metadata": {
        "id": "ZAiKYwqaWBgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only basic tensor operations\n",
        "**Here you may only use basic tensor operations from the `torch` module: no `torch.nn`, `torch.nn.F` or others.**"
      ],
      "metadata": {
        "id": "t3I13Fb_XqhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(object):\n",
        "    \"\"\"\n",
        "    ReLU non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "        # Define a cache variable because some of the forward pass values\n",
        "        # would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of ReLU non-linear activation function: y=max(0,x). Store\n",
        "        input tensor as cache variable.\n",
        "        \n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #                   TODO: Update cache variable.                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y =   # forward pass, Hint: use torch.clamp function\n",
        "        self.cache = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of ReLU non-linear activation function: return downstream\n",
        "        gradient dx.\n",
        "        \n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Making sure that we don't modify the incoming upstream gradient\n",
        "        dupstream = dupstream.clone()\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y = \n",
        "        dx = \n",
        "        dx[y == 0] = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Sigmoid(object):\n",
        "    \"\"\"\n",
        "    Sigmoid non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__()\n",
        "\n",
        "        # Define a cache variable because some of the forward pass value\n",
        "        # would be used during backward pass.\n",
        "        self.cache = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Sigmoid non-linear activation function: y=1/(1+exp(-x)).\n",
        "        Store input tensor as cache variable.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################        \n",
        "\n",
        "        y = \n",
        "        self.cache = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of Sigmoid non-linear activation function: return\n",
        "        downstream gradient dx.\n",
        "        \n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y = \n",
        "        dx = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "oImYaIZKWTQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define layer dimensions and dummy input\n",
        "n_samples, in_features = 2, 3\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "\n",
        "# Define upstream gradient dL/dy as ones\n",
        "dy = torch.randn((n_samples, in_features))\n",
        "print('dy:', dy)\n",
        "\n",
        "relu = ReLU()\n",
        "sigmoid = Sigmoid()\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Perform a forward and backward pass with the ReLU and       #\n",
        "#                          Sigmoid layers                              #\n",
        "########################################################################\n",
        "\n",
        "y_relu = \n",
        "dx_relu = \n",
        "\n",
        "y_sigmoid = \n",
        "dx_sigmoid = \n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# What will be the shapes of output tensors y_relu and y_sigmoid?\n",
        "print('Shape of ouput tensors y_relu and y_sigmoid:', y_relu.shape, y_sigmoid.shape)\n",
        "\n",
        "# What will be the shapes of gradient tensors dx_relu and dx_sigmoid?\n",
        "print('Output shapes from ReLU and Sigmoid: ', dx_relu.shape, dx_sigmoid.shape)"
      ],
      "metadata": {
        "id": "S4E6rPniX4I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PyTorch NN module\n",
        "A list of all available non-linearities in PyTorch can be found [[here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)]."
      ],
      "metadata": {
        "id": "aIYxpQkyZfHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the input tensor so we can use x for all other layers\n",
        "# without overwriting its gradients\n",
        "x_relu = x.clone()\n",
        "x_sigmoid = x.clone()\n",
        "# Enable requires_grad\n",
        "x_relu.requires_grad = True\n",
        "x_sigmoid.requires_grad = True\n",
        "\n",
        "print('dy: ', dy)\n",
        "print('---')\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Perform a forward and backward pass with the ReLU and       #\n",
        "#                          Sigmoid functions                           #\n",
        "########################################################################\n",
        "\n",
        "# ReLU forward pass\n",
        "torch_relu = \n",
        "torch_y_relu = \n",
        "# Perform backward pass\n",
        "\n",
        "# Sigmoid forward pass\n",
        "torch_sigmoid = \n",
        "torch_y_sigmoid = \n",
        "# Perform backward pass\n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# Compare outputs using torch.allclose\n",
        "dx_relu_same = torch.allclose(dx_relu, x_relu.grad)\n",
        "print('dx_relu identical: ', dx_relu_same) # Make sure dx_relu_same = True\n",
        "\n",
        "\n",
        "dx_sigmoid_same = torch.allclose(dx_sigmoid, x_sigmoid.grad)\n",
        "print('dx_sigmoid identical: ', dx_sigmoid_same) # Make sure dx_sigmoid_same = True\n"
      ],
      "metadata": {
        "id": "7hluLYZMZUOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network class"
      ],
      "metadata": {
        "id": "JOAh9RzYaP40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only basic tensor operations\n",
        "**Here you may only use basic tensor operations from the `torch` module: no `torch.nn`, `torch.nn.F` or others.**"
      ],
      "metadata": {
        "id": "9FgilJdablMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(object):\n",
        "    \"\"\"\n",
        "    Neural network object containing layers.\n",
        "    \n",
        "    Args:\n",
        "        layers: list of layers in neural network\n",
        "    \"\"\"\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "        # Initialize params\n",
        "        self.reset_params()\n",
        "\n",
        "    def reset_params(self, std=1.):\n",
        "        \"\"\"\n",
        "        Reset network parameters. Applies `init_params` to all layers with\n",
        "        learnable parameters.\n",
        "        \n",
        "        Args:\n",
        "            std: Standard deviation of Gaussian distribution (default: 0.1)\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'init_params'):\n",
        "                layer.init_params(std=std)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs forward pass through all layers of the network.\n",
        "        \n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            x: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        x = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Performs backward pass through all layers of the network.\n",
        "        \n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        dx = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx\n",
        "    \n",
        "    def optimizer_step(self, lr):\n",
        "        \"\"\"\n",
        "        Updates network weights by performing a step in the negative gradient\n",
        "        direction in each layer. The step size is determined by the learning\n",
        "        rate.\n",
        "        \n",
        "        Args:\n",
        "            lr: Learning rate to use for update step.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        #    Hint: use `if hasattr(layer, 'weight')` to check if a layer has   #\n",
        "        #                       trainable parameters.                          #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################"
      ],
      "metadata": {
        "id": "lsIb8VSLaT8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define layer dimensions\n",
        "n_samples, in_features, hidden_dim, out_features = 2, 3, 5, 4\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          ReLU(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Create a network from the Net object class above            #\n",
        "#                 and do a forward and a backward pass                 #\n",
        "########################################################################\n",
        "\n",
        "# Initialize network\n",
        "net = \n",
        "\n",
        "# Do forward pass\n",
        "y = \n",
        "\n",
        "# Gradient of y w.r.t. y is 1\n",
        "dy = \n",
        "\n",
        "# Do backward pass\n",
        "dx = \n",
        "\n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# What will be the shape of output tensor y?\n",
        "print('Shape of ouput tensor y:', y.shape)\n",
        "\n",
        "# What will be the shape of gradient x?\n",
        "print('Shape of gradient x:', dx.shape)"
      ],
      "metadata": {
        "id": "48LxgvD0awv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using PyTorch NN module"
      ],
      "metadata": {
        "id": "qpmQDjuxbeT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchNet(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
        "    pass implemented in forward.\n",
        "    \n",
        "    Args:\n",
        "        in_features: number of features in input layer\n",
        "        hidden_dim: number of features in hidden dimension\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features, hidden_dim, out_features):\n",
        "        super(TorchNet, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #         TODO:  Initialize layers as the previous neural net          #\n",
        "        #                       having Linear, ReLU, Linear                    #\n",
        "        ########################################################################\n",
        "\n",
        "        self.layer1 = \n",
        "        self.relu = \n",
        "        self.layer2 = \n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "        x = \n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "        return x\n",
        "\n",
        "# Initialize Pytorch network\n",
        "torch_net = TorchNet(in_features, hidden_dim, out_features)\n",
        "print(torch_net)"
      ],
      "metadata": {
        "id": "D7tCKFVnazWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the parameters from our model into the Pytorch model\n",
        "torch_net.layer1.weight = nn.Parameter(net.layers[0].weight.T) # transpose weight by .T\n",
        "torch_net.layer1.bias = nn.Parameter(net.layers[0].bias)\n",
        "torch_net.layer2.weight = nn.Parameter(net.layers[2].weight.T) # transpose weight by .T\n",
        "torch_net.layer2.bias = nn.Parameter(net.layers[2].bias)\n",
        "\n",
        "# Make copy of x\n",
        "torch_x = x.clone()\n",
        "torch_x.requires_grad = True\n",
        "\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Perform a forward and a backward pass                       #\n",
        "########################################################################\n",
        "\n",
        "# Perform forward pass\n",
        "torch_y = \n",
        "\n",
        "# Perform backward pass\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# What will be the shape of output tensor torch_y?\n",
        "print('Shape of ouput tensor y:', torch_y.shape)\n",
        "\n",
        "# What will be the shape of gradient x?\n",
        "print('Shape of gradient x :', torch_x.grad.shape)\n",
        "\n",
        "# Compare gradients using torch.allclose\n",
        "dx_same = torch.allclose(dx, torch_x.grad)\n",
        "print('Gradients identical: ', dx_same)"
      ],
      "metadata": {
        "id": "PpMziz1zcq5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax and Cross entropy loss\n",
        "In a classification task we would like to interpret the output of our network as class probabilities, i.e. for all inputs $x$ and all classes $k$ we should have $$0 \\leq P(Y=k|X=x) \\leq 1, \\qquad \\sum_kP(Y=k|X=x) = 1.$$\n",
        "This can be achieved by normalizing the logits $z$ using the **Softmax layer**:\n",
        "$$P(Y=k|X=x) = \\text{softmax}(\\mathbf{z})_k = \\frac{\\exp{z_k}}{\\sum_i^K \\exp{z_i}},$$\n",
        "where $K$ is the number of classes.\n",
        "\n",
        "The **Cross Entropy (CE)** loss ios defined as:\n",
        "$$H(y,p) = -\\sum_i^K y_i \\log(p_i),$$\n",
        "where $y$ is the one-hot encoded label and $p$ is the class probability vector from the Softmax layer.\n",
        "<!-- Using the CE loss with the Softmax layer is effective as the $\\log$ in the CE loss can undo the $\\exp$ in the Softmax layer:\n",
        "$$\\log \\text{softmax}(\\mathbf{z})_k = z_k - \\log \\sum_i^K \\exp z_i.$$\n",
        "Because now $z_k$ has a direct contribution to the loss it can never saturate and the gradients will never become too small. -->\n",
        "\n",
        "The Softmax layer and Cross Entropy loss are used together so often that they are implemented as a single function in Pytorch as `torch.nn.CrossEntropyLoss`."
      ],
      "metadata": {
        "id": "F0-7myfXlzFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Softmax(z):\n",
        "    \"\"\"\n",
        "    Computes softmax output for each sample in batch.\n",
        "\n",
        "    Args:\n",
        "      z: Tensor of logits, dimension [batch, classes].\n",
        "\n",
        "    Return:\n",
        "      p: Softmax probability distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################################################################\n",
        "    #                  TODO: Implement Softmax function.                   #\n",
        "    ########################################################################\n",
        "\n",
        "\n",
        "    p = \n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    return p\n",
        "\n",
        "def CrossEntropyLoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes softmax output and cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "      y_true: Tensor containing true labels.\n",
        "      y_pred: Tensor containing predictions.\n",
        "\n",
        "    Return:\n",
        "      loss: Cross-entropy loss.\n",
        "      dy_pred: Gradient of loss w.r.t. y_pred.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate softmax using previously defined function\n",
        "    softmax = Softmax(y_pred)\n",
        "\n",
        "    # Convert one-hot vector to class id\n",
        "    y_true = torch.argmax(y_true, axis=1)\n",
        "    # Get number of samples in batch\n",
        "    n = y_true.shape[0]\n",
        "    # Calculate cross entropy loss between y_true and y_pred\n",
        "    log_likelihood = -torch.log(softmax[torch.arange(n),y_true])\n",
        "    # Average over all samples\n",
        "    loss = torch.mean(log_likelihood)\n",
        "\n",
        "    # Caculate the gradient \n",
        "    grad = softmax\n",
        "    softmax[torch.arange(n), y_true] -= 1\n",
        "    grad /= n\n",
        "\n",
        "    return loss, grad"
      ],
      "metadata": {
        "id": "pakibtubda4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST digit classification"
      ],
      "metadata": {
        "id": "TV-tyYR_nB95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
        "# mean and dividing by std.\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Get data from torchvision.datasets\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loaders used to iterate through dataset\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "# Show some example images\n",
        "fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
        "for i in range(25):\n",
        "    x, _ = test_data[i]\n",
        "    ax = axs[i // 5][i % 5]\n",
        "    ax.imshow(x.view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwwwZdkinMgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#             TODO: Define appropriate network dimensions.             #\n",
        "########################################################################\n",
        "\n",
        "in_features, hidden_dim, out_features = 28*28, 30, 10\n",
        "\n",
        "########################################################################\n",
        "#                          END OF YOUR CODE                            #\n",
        "########################################################################\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 5e-1  # step size for gradient descent\n",
        "epochs = 10  # how many times to iterate through the intire training set\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          Sigmoid(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "# Initialize network\n",
        "net = Net(layers)\n",
        "\n",
        "# Define list to store loss of each iteration\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training loop\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        # Flatten input to 1D tensor\n",
        "        x_batch = x_batch.flatten(start_dim=1)\n",
        "        # Convert labels to one-hot encoding\n",
        "        y_batch = nn.functional.one_hot(y_batch, num_classes=10)\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #      TODO: Perform forward pass and calculate Cross Entropy loss     #\n",
        "        #                between prediction and labels .                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y_pred = \n",
        "        loss, grad = \n",
        "        train_losses.append(loss)\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #            TODO: Perform backward pass and optimizer step.           #\n",
        "        ########################################################################\n",
        "        \n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        # Calculate accuracy of prediction\n",
        "        correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_batch, axis=1)\n",
        "        train_accs.append(torch.sum(correct)/len(y_pred))\n",
        "\n",
        "        # Print progress\n",
        "        if i % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, i * len(x_batch), len(train_loader.dataset),\n",
        "                100. * i / len(train_loader), loss))\n",
        "\n",
        "    # Validation loop    \n",
        "    test_loss = 0\n",
        "    total_correct = 0\n",
        "    for x_batch, y_batch in test_loader:\n",
        "        # Flatten input to 1D tensor\n",
        "        x_batch = x_batch.flatten(start_dim=1)\n",
        "        # Convert labels to one-hot encoding\n",
        "        y_batch = nn.functional.one_hot(y_batch, num_classes=10)\n",
        "\n",
        "        # Perform forward pass with x_batch as input and y_pred as output variables\n",
        "        y_pred = net.forward(x_batch)\n",
        "\n",
        "        ########################################################################\n",
        "        #   TODO: Calculate Cross Entropy loss between prediction and labels.  #\n",
        "        ########################################################################\n",
        "\n",
        "        loss, grad = \n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        # Keep track of total loss over test set\n",
        "        test_loss += loss\n",
        "\n",
        "        # Calculate accuracy of prediction\n",
        "        correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_batch, axis=1)\n",
        "        total_correct += torch.sum(correct)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * total_correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, total_correct, len(test_loader.dataset), test_acc))\n",
        "    \n",
        "# Plot training curves\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(train_accs)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "g77v94eKnOWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}